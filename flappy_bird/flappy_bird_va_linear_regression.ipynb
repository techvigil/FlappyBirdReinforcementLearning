{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flappy Bird Value Approximator: Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function Approximation\n",
    "- Function Approximators: Linear Combinations and Neural Network\n",
    "- Increment Methods: Stochastic Gradient Descent Prediction and Control \n",
    "- Batch Methods: Least Squares Prediction and Control, Experience Replay\n",
    "\n",
    "## Types of Function Approximators\n",
    "- There are many function Approximators – supervised ML algorithms: Linear combinations of features, Neural Network, Decision Tree, etc.\n",
    "- RL can get better benefit from differential function Approximators like Linear and Neural Network algorithms\n",
    "- Incremental methods update the weights on each sample while batch does an updated on each epoch (batch).\n",
    "- Stochastic Gradient Descent (SCD) is an incremental and iterative optimization algorithm to find values of parameters (weights) of a function that minimizes  cost function. \n",
    "- Least Squares method is a form of mathematical regression analysis that finds the line of best fit for a dataset, providing a - visual demonstration of the relationship between the data points.\n",
    "\n",
    "## Linear Approximating the Q Function\n",
    "\\begin{equation}\n",
    "\\large\n",
    "Q(s,a) = w_1*f_1(s,a) + w_2*f_2(s,a) + w_n*f_n(s,a)\n",
    "\\end{equation}\n",
    "![title](images/va_linear.png)\n",
    "\n",
    "\n",
    "## Q-Learning with Linear Approximation\n",
    "\n",
    "- Step 1: Start with initial parameter values\n",
    "- Step 2: Take action a according to an explore/exploit policy, transitioning from s to s’\n",
    "- Step 3: Perform TD update for each parameter\n",
    "    \\begin{equation}\n",
    "\\large\n",
    "\\theta_i \\leftarrow \\theta_i + \\alpha [R(s) + \\beta * max_{a'}\\hat{Q_\\theta}(s', a') - \\hat{Q_\\theta}(s, a)]* f_i(s,a)\n",
    "\\end{equation}\n",
    "- Step 4: Go to Step 2\n",
    "\n",
    "TD converges close to minimum error solution\n",
    "\n",
    "Q-Learning can diverge. Converges under some conditions.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Linear function approximation that approximates the underlying value function **V(s)** supposed to be linear; the output would be linear combination of its features. \n",
    "\n",
    "Gradient Descent is one of the simplest form of linear function approximation. In gradient descent methods, the feature vector is a column vector which is expected to be differentiable function. Gradient Descent methods minimize error on the observed samples by adjusting the parameters after each sample by a small amount in the direction that would reduce the error between the actual and expected. However in Reinforcement Learning with continuous space like flappy bird, gradient descent, does not guarantee the convergence. \n",
    "\n",
    "\n",
    "## Radial Basis Function (RBF)\n",
    "\n",
    "Radial Basis Function is useful for Reinforcement Environments with continuous state space and ideal for flappy bird like environments where the number of features are small. Radial Basis an extension to tile encoding where the the state space (features) is grouped into tiles where each tile represents a binary feature (0 or 1). In RBF, each feature can have have any value between 0 and 1, instead of 0 or 1. Due to this RBF can generate better approximate functions that are smoothly differentiable and also are easy to calculate. Hence it can converge better compared to Gradient Descent or Tile based value approximators. RBF can get complex for non-linear features. RBF can hold the key for reinforcement learning given that it does not discard the previously learned information while gathering the new information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook function to disable cell-level scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/*jupyter functionality - remove the cell-level scrollbar*/\n",
       "\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/*jupyter functionality - remove the cell-level scrollbar*/\n",
    "\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run plotting.py\n",
    "%run flappy_bird_env_open_ai_gym.py #open AI gym clone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datetime\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "\n",
    "#from lib import plotting\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = FlappyBirdEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "# We use a few samples from the observation space to do this\n",
    "sample_size = 100000 #10000\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(sample_size)])\n",
    "#observation_examples = states\n",
    "print(observation_examples.shape)\n",
    "#print(observation_examples)\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Used to convert a state to a featurized representation.\n",
    "# We use RBF kernels with different variances to cover different parts of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "featurizer.fit(scaler.transform(observation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # We create a separate model for each action in the environment's\n",
    "        # action space. Alternatively we could somehow encode the action\n",
    "        # into the features, but this way it's easier to code up.\n",
    "        self.models = []\n",
    "        for _ in range(env.action_space.n):\n",
    "            model = SGDRegressor(learning_rate=\"constant\")\n",
    "            # We need to call partial_fit once to initialize the model\n",
    "            # or we get a NotFittedError when trying to make a prediction\n",
    "            # This is quite hacky.\n",
    "            model.partial_fit([self.featurize_state(env.reset(2))], [0])\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def featurize_state(self, state):\n",
    "        \"\"\"\n",
    "        Returns the featurized representation for a state.\n",
    "        \"\"\"               \n",
    "        scaled = scaler.transform([state])       \n",
    "        featurized = featurizer.transform(scaled)        \n",
    "        return featurized[0]       \n",
    "    \n",
    "    def predict(self, s, a=None):\n",
    "        \"\"\"\n",
    "        Makes value function predictions.\n",
    "        \n",
    "        Args:\n",
    "            s: state to make a prediction for\n",
    "            a: (Optional) action to make a prediction for\n",
    "            \n",
    "        Returns\n",
    "            If an action a is given this returns a single number as the prediction.\n",
    "            If no action is given this returns a vector or predictions for all actions\n",
    "            in the environment where pred[i] is the prediction for action i.\n",
    "            \n",
    "        \"\"\"\n",
    "                \n",
    "        # TODO: Implement this!\n",
    "        if a is not None:\n",
    "            prediction = self.models[a].predict([self.featurize_state(s)])\n",
    "            return prediction[0]\n",
    "        \n",
    "        else:\n",
    "            predictions = np.array([self.models[i].predict([self.featurize_state(s)]) for i in range(env.action_space.n)])\n",
    "            return predictions.reshape(-1)\n",
    "            \n",
    "    def update(self, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator parameters for a given state and action towards\n",
    "        the target y.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this!\n",
    "        self.models[a].partial_fit([self.featurize_state(s)], [y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(observation)\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save_stats method is used to capture the output of all the episodes with metrics: algorithm, duration, episode, reward and score.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start  = datetime.datetime.now()\n",
    "#data = []\n",
    "def save_stats(episode,data,score):\n",
    "\n",
    "    algorithm = \"linear\"\n",
    "    \n",
    "#    data.append(json.dumps({ \"algorithm\": algorithm, \n",
    "#                \"duration\":  \"{}\".format(duration), \n",
    "#                \"episode\":   episode, \n",
    "#                \"reward\":    reward, \n",
    "#                \"score\":     score}))\n",
    "        \n",
    "    if (score >= 50):\n",
    "        print(data)\n",
    "\n",
    "    \n",
    "\n",
    "    file_name = 'data/stats_flappy_bird_{}.json'.format(algorithm)\n",
    "            \n",
    "            # delete the old file before saving data for this session\n",
    "    if episode == 0 and os.path.exists(file_name): os.remove(file_name)\n",
    "                \n",
    "            # open the file in append mode to add more json data\n",
    "    file = open(file_name, 'a+')  \n",
    "    for item in data:\n",
    "        file.write(item)  \n",
    "        file.write(\",\")\n",
    "            #end for\n",
    "    file.close()\n",
    "            \n",
    "        #    data = []\n",
    "        #end if   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_learning(env, estimator, num_episodes, discount_factor=0.99, epsilon=0.3, epsilon_decay=0.0001):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator: Action-Value function estimator\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "        epsilon_decay: Each episode, epsilon is decayed by this factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.now()\n",
    "    data = []\n",
    "    nA = env.action_space.n\n",
    "    algorithm = \"linear\"\n",
    "    EpisodeStats = collections.namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    episode_reward = 0\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        # The policy we're following\n",
    "        policy = make_epsilon_greedy_policy(\n",
    "            estimator, epsilon * epsilon_decay**i_episode, nA)\n",
    "        \n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        # Also print reward for last episode\n",
    "        last_reward = stats.episode_rewards[i_episode - 1]\n",
    "        #print(\"\\rEpisode {}/{} ({})\".format(i_episode + 1, num_episodes, last_reward))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        episode_reward += last_reward\n",
    "        \n",
    "        # TODO: Implement this!\n",
    "        state = env.reset(2)\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # sample the action from the epsilon greedy policy\n",
    "            action = np.random.choice(nA, p=policy(state))\n",
    "            # Perform the action -> Get the reward and observe the next state\n",
    "            new_state, reward, terminated, _ = env.step(action,2)\n",
    "            env.render()\n",
    "            new_action = np.random.choice(nA, p=policy(new_state))\n",
    "                        \n",
    "            #stats.episode_rewards[i_episode] += reward\n",
    "            #stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            q_values_new_state = estimator.predict(new_state)\n",
    "\n",
    "            # value that we should have got\n",
    "            # The Q-learning target policy is a greedy one, hence the `max`\n",
    "            td_target = reward + discount_factor * np.max(q_values_new_state)\n",
    "            estimator.update(state, action, td_target)            \n",
    "            \n",
    "            # update current state\n",
    "            state = new_state\n",
    "        \n",
    "            if terminated:\n",
    "                break\n",
    "        \n",
    "        duration = datetime.datetime.now() - start \n",
    "        data.append(json.dumps({ \"algorithm\": algorithm, \n",
    "                \"duration\":  \"{}\".format(duration), \n",
    "                \"episode\":   i_episode, \n",
    "                \"reward\":    episode_reward, \n",
    "                \"score\":     env.score}))\n",
    "        \n",
    "        if (i_episode% 500 == 0):\n",
    "            save_stats(i_episode,data,env.score)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheru\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "estimator = Estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheru\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "max_episodes = 25000\n",
    "if __name__ ==\"__main__\":\n",
    "    stats = q_learning(env, estimator, max_episodes, epsilon=0.1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
