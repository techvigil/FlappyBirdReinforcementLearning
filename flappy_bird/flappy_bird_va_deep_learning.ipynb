{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flappy Bird Value Approximator: Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function Approximation\n",
    "- Function Approximators: Linear Combinations and Neural Network\n",
    "- Increment Methods: Stochastic Gradient Descent Prediction and Control \n",
    "- Batch Methods: Least Squares Prediction and Control, Experience Replay\n",
    "\n",
    "## Types of Function Approximators\n",
    "- There are many function Approximators – supervised ML algorithms: Linear combinations of features, Neural Network, Decision Tree, etc.\n",
    "- RL can get better benefit from differential function Approximators like Linear and Neural Network algorithms\n",
    "- Incremental methods update the weights on each sample while batch does an updated on each epoch (batch).\n",
    "- Stochastic Gradient Descent (SCD) is an incremental and iterative optimization algorithm to find values of parameters (weights) of a function that minimizes  cost function. \n",
    "- Least Squares method is a form of mathematical regression analysis that finds the line of best fit for a dataset, providing a - visual demonstration of the relationship between the data points.\n",
    "\n",
    "## Neural Network Approximating the Q Function\n",
    "\n",
    "![title](images/va_nonlinear.png)\n",
    "![title](images/va_nonlinear_z.png)\n",
    "\n",
    "\n",
    "## Q-Learning with Non-Linear Approximation\n",
    "\n",
    "- Step 1: Start with initial parameter values\n",
    "- Step 2: Take action a according to an explore or exploit policy, transitioning from s to s’\n",
    "- Step 3: Perform TD update for each parameter\n",
    "     \\begin{equation}\n",
    "\\large\n",
    "\\theta_i \\leftarrow \\theta_i + \\alpha [R(s) + \\beta * max_{a'}\\hat{Q_\\theta}(s', a') - \\hat{Q_\\theta}(s, a)]* \\frac{\\partial\\hat{Q_\\theta}(s,a)}{\\partial\\theta_i}\n",
    "\\end{equation}\n",
    "- Step 4: Go to Step 2\n",
    "\n",
    "Typically the space has many local minima and we no longer guarantee convergence, often works well in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Concepts\n",
    "\n",
    "- Perceptron, the first generation neural network, created a simple mathematical model or a function, mimicking neuron – the basic unit of brain\n",
    "- Sigmoid Neuron improved learning by giving some weightage to the input\n",
    "- Neural Network is a directed graph, organized by layers and layers are created by number of interconnected neurons (nodes)\n",
    "- Typical neural network contains three layers: input, hidden and output. If the hidden layers are more than one, then it is called deep neural network\n",
    "- Actual processing happens in hidden layers where each neuron acts as an activation function to process the input (from previous layers)\n",
    "- The performance of neural network is measured using cost or error function and the dependent weight functions\n",
    "- Forward and backward-propagation are two techniques, neural network users repeatedly until all the input variables are adjusted or calibrated to predict accurate output.\n",
    "- During, forward-propagation, information moves in forward direction and passes through all the layers by applying certain weights to the input parameters. Back-propagation method minimizes the error in the weights by applying an algorithm called gradient descent at each iteration step.\n",
    "\n",
    "![title](images/nn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "\n",
    "- Deep Learning is an advanced neural network with multiple hidden layers that can work with supervised or unsupervised datasets.\n",
    "- Deep Learning vectorizes the input and converts it into output vector space by decomposing complex geometric and polynomial equations into a series of simple transformations. These transformations go through neuron activation functions at each layer parameterized by input weights.\n",
    "- Convolutional Neural Network (CNN) consists of (1) convolutional layers - to identify the features using weights and biases, followed by (2) fully connected layers - where each neuron is connected from all the neurons of previous layers - to provide nonlinearity, sub-sampling or max-pooling, performance and control data overfitting. Examples include: image and voice recognition.\n",
    "- Recursive Neural Network (RNN) is, another type of Deep Learning, that uses same shared feature weights recursively for processing sequential data, emitted by sensors or the way spoken words are processed in NLP, to produce arbitrary size input and output vectors. Long Short Term Memory (LSTM) is an advanced RNN to learn and remember longer sequences by composing series of repeated modules of neural network. \n",
    "\n",
    "![title](images/deep_nn.png)\n",
    "![title](images/cnn.png)\n",
    "![title](images/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Sharing and Experience Replay\n",
    "\n",
    "- **Weight Sharing**: Convolutional Neural Network shares weights between local regions Recurrent Neural Network shares weights between time-steps\n",
    "\n",
    "\n",
    "- **Experience Replay**: Store experience (S, A, R, Snext) in a replay buffer and sample mini-batches from it to train the network. This de-correlates the data and leads to better data efficiency. In the beginning, the replay buffer is filled with random experience.Better convergence behavior when training a function approximator. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Network (DQN)\n",
    "\n",
    "- Step 1: Take action at according to e-greedy policy\n",
    "- Step 2: Store transition (st, at, rt+1, st+1) in replay memory D\n",
    "- Step 3: Sample random mini-batch of transitions (s, a, r, s’) from D\n",
    "- Step 4: Compute Q-learning targets w.r.t old, fixed parameters w—\n",
    "- Step 5: Optimize MSE (mean squared error) between Q-network and Q-learning targets\n",
    "    \\begin{equation}\n",
    "\\Large\n",
    "\\mathcal{L_i(w_i)} = \\mathbb{E}_{s,a,r,s' \\tilde{} D_i}[(r + \\gamma * max_{a'} Q(s', a';w_i^-) - Q(s,a; w_i))^2]\n",
    "\\end{equation}\n",
    "- Step 6: Using variant of stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some key aspects of the implementation:\n",
    "\n",
    "Libraries used: Keras with TensorFlow (**GPU version**) and trained for several hours in Azure Windows Environment.\n",
    "\n",
    "To scale the implementation, we pre-process the images by converting color images to grayscale and then crop the images to 80X80 pixels. And then stack 4 frames together so that the flappy bird velocity is inferred properly.\n",
    "\n",
    "- The input to the neural network consists of an 4x80x80 images. \n",
    "- The first hidden layer convolves 32 filters of 8 x 8 with stride 4 and applies ReLU activation function. \n",
    "- The 2nd layer convolves a 64 filters of 4 x 4 with stride 2 and applies ReLU activation function. \n",
    "- The 3rd layer convolves a 64 filters of 3 x 3 with stride 1 and applies ReLU activation function. \n",
    "- The final hidden layer is fully-connected consisted of 512 rectifier units. \n",
    "- The output layer is a fully-connected linear layer with a single output for each valid action.  \n",
    "\n",
    "![title](images/flappy_dqn.png)\n",
    "Image Source: https://github.com/yenchenlin/DeepLearningFlappyBird\n",
    "\n",
    "**Convolution** actually helps computer to learn higher features like edges and shapes. The example below shows how the edges are stand out after a convolution filter is applied.\n",
    "\n",
    "**Keras** makes it very easy to build convolution neural network. However, there are few things to track:\n",
    "\n",
    "- A) It is important to choose a right initialization method. I choose normal distribution with sigma(σ) =0.01. init=lambda shape, name: normal(shape, scale=0.01, name=name)\n",
    "\n",
    "- B) The ordering of the dimension is important, the default setting is 4x80x80 (Theano setting), so if your input is 80x80x4 (Tensorflow setting) then you are in trouble because the dimension is wrong. Alert: If your input dimension is 80x80x4 (Tensorflow setting) you need to set dim_ordering = tf (tf means tensorflow, th means theano)\n",
    "\n",
    "- C) In Keras, subsample=(2,2) means you down sample the image size from (80x80) to (40x40). In ML literature it is often called “stride”\n",
    "\n",
    "- D) We have used an adaptive learning algorithm called ADAM to do the optimization. The learning rate is 1-e6.\n",
    "\n",
    "**Experience Relay:**\n",
    "\n",
    "It was found that approximation of Q-value using non-linear functions like neural network is not very stable. During the game-play all the episode (s,a,r,s′) are stored in replay memory D. When training the network, random mini-batches from the replay memory are used instead of most the recent transition, which will greatly improve the stability.\n",
    "\n",
    "## Policy Gradient (PG)\n",
    "\n",
    "Policy Gradient algorithms optimize the parameters of a policy by following the gradients toward higher rewards. One popular class of PG algorithms, called REINFORCE algorithms, was introduced back in 1992 by Ronald Williams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook function to disable cell-level scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.transform import rotate\n",
    "from skimage.viewer import ImageViewer\n",
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "#import wrapped_flappy_bird as game\n",
    "from flappy_bird_env import * \n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import json\n",
    "from keras.initializers import normal, identity\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run flappy_bird_env_open_ai_gym.py #open AI gym clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAME = 'bird' # the name of the game being played for log files\n",
    "CONFIG = 'nothreshold'\n",
    "ACTIONS = 2 # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVATION = 1000. # timesteps to observe before training\n",
    "EXPLORE = 1000000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.08 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "img_rows , img_cols = 80, 80\n",
    "#Convert image into Black and white\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    \n",
    "    #80*80*4\n",
    "    model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',\n",
    "                                input_shape=(img_rows,img_cols,img_channels))) \n",
    "    #hidden layers\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(2))\n",
    "   \n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNetwork(model,args):\n",
    "    # open up a game state to communicate with emulator\n",
    "    max_episode=2000000\n",
    "    env = FlappyBirdEnv()\n",
    "    start = datetime.datetime.now()\n",
    "    algorithm = 'DQN'\n",
    "    data = []\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque()\n",
    "\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    #do_nothing = np.zeros(ACTIONS)\n",
    "    #do_nothing[0] = 1\n",
    "\n",
    "    if args['mode'] == 'Run':\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"data/model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        model.load_weights(\"data/model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = INITIAL_EPSILON\n",
    "\n",
    "    t = 0\n",
    "    for t in range( max_episode):\n",
    "        x_t = env.reset(return_type=3)\n",
    "  \n",
    "        x_t = skimage.color.rgb2gray(x_t)\n",
    "        x_t = skimage.transform.resize(x_t,(80,80))\n",
    "        x_t = skimage.exposure.rescale_intensity(x_t,out_range=(0,255))\n",
    "\n",
    "        x_t = x_t / 255.0\n",
    "\n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    #print (s_t.shape)\n",
    "\n",
    "    #In Keras, need to reshape\n",
    "        s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
    "        \n",
    "        #loss = 0\n",
    "        #Q_sa = 0\n",
    "        terminal=False\n",
    "        while not terminal:\n",
    "            loss = 0\n",
    "            Q_sa = 0\n",
    "            #action_index = 0\n",
    "            r_t = 0\n",
    "            a_t = 0\n",
    "        #choose an action epsilon greedy\n",
    "            if t % FRAME_PER_ACTION == 0:\n",
    "                if random.random() <= epsilon:\n",
    "                    print(\"----------Random Action----------\")\n",
    "                    action = random.randrange(ACTIONS)\n",
    "                    a_t = action\n",
    "                    #print(\"a_t\", a_t)\n",
    "                else:\n",
    "                    q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                    max_Q = np.argmax(q)\n",
    "                    action = max_Q\n",
    "                    a_t = action\n",
    "                    #print(\"a_t\", a_t)\n",
    "\n",
    "        #We reduced the epsilon gradually\n",
    "            if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "                epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "            x_t1_colored, r_t, terminal,_ = env.step(a_t,return_type=3)\n",
    "            env.render()\n",
    "            x_t1 = skimage.color.rgb2gray(x_t1_colored)\n",
    "            x_t1 = skimage.transform.resize(x_t1,(80,80))\n",
    "            x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))\n",
    "\n",
    "\n",
    "            x_t1 = x_t1 / 255.0\n",
    "\n",
    "\n",
    "            x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
    "            s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "\n",
    "        # store the transition in D\n",
    "            D.append((s_t, action, r_t, s_t1, terminal))\n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "            if t > OBSERVE:\n",
    "            #sample a minibatch to train on\n",
    "                minibatch = random.sample(D, BATCH)\n",
    "\n",
    "            #Now we do the experience replay\n",
    "                state_t, action_t, reward_t, state_t1, done = zip(*minibatch)\n",
    "                state_t = np.concatenate(state_t)\n",
    "                state_t1 = np.concatenate(state_t1)\n",
    "                targets = model.predict(state_t)\n",
    "                Q_sa = model.predict(state_t1)\n",
    "                #print (\"Q_sa\", Q_sa)\n",
    "                #print (\"Max Q_sa\",np.max(Q_sa, axis=1))\n",
    "                targets[range(BATCH), action_t] = reward_t + GAMMA*np.max(Q_sa, axis=1)*np.invert(done)\n",
    "\n",
    "                loss += model.train_on_batch(state_t, targets)\n",
    "            \n",
    "     \n",
    "            s_t = s_t1\n",
    "            #if terminal:\n",
    "            #    break\n",
    "            \n",
    "        t = t + 1\n",
    "        \n",
    "        \n",
    "        duration = datetime.datetime.now() - start \n",
    "        \n",
    "        if (env.score >= 10):\n",
    "            print(\"Duration: {} Episode {} Score: {}\".format(duration, \n",
    "                                                                t, \n",
    "                                                                env.score))\n",
    "        \n",
    "        data.append(json.dumps({ \"algorithm\": algorithm, \n",
    "                    \"duration\":  \"{}\".format(duration), \n",
    "                    \"episode\":   t, \n",
    "                    \"reward\":    r_t, \n",
    "                    \"score\":     env.score}))\n",
    "        \n",
    "        if (len(data) == 500):\n",
    "            file_name = 'data/stats_flappy_bird_{}.json'.format(algorithm)\n",
    "            \n",
    "            # delete the old file before saving data for this session\n",
    "            #if t == 1 and os.path.exists(file_name): os.remove(file_name)\n",
    "                \n",
    "            # open the file in append mode to add more json data\n",
    "            file = open(file_name, 'a+')  \n",
    "            for item in data:\n",
    "                file.write(item)  \n",
    "                file.write(\",\")\n",
    "            #end for\n",
    "            file.close()\n",
    "            \n",
    "            data = []\n",
    "            \n",
    "        # save progress every 10000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            with open(\"data/model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
    "            \"/ EPSILON\", epsilon, \"/ ACTION\", action, \"/ REWARD\", r_t, \\\n",
    "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "        print(\"Episode finished!\")\n",
    "        print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def playGame(args):\n",
    "    model = buildmodel()\n",
    "    trainNetwork(model,args)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Description of your program')\n",
    "    parser.add_argument('-m','--mode', help='Train / Run', required=True)\n",
    "    args = vars(parser.parse_args())\n",
    "    playGame(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    from keras import backend as K\n",
    "    K.set_session(sess)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
