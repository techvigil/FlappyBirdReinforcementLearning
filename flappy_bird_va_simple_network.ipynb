{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flappy Bird Value Approximator: Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function Approximation\n",
    "- Function Approximators: Linear Combinations and Neural Network\n",
    "- Increment Methods: Stochastic Gradient Descent Prediction and Control \n",
    "- Batch Methods: Least Squares Prediction and Control, Experience Replay\n",
    "\n",
    "## Types of Function Approximators\n",
    "- There are many function Approximators – supervised ML algorithms: Linear combinations of features, Neural Network, Decision Tree, etc.\n",
    "- RL can get better benefit from differential function Approximators like Linear and Neural Network algorithms\n",
    "- Incremental methods update the weights on each sample while batch does an updated on each epoch (batch).\n",
    "- Stochastic Gradient Descent (SCD) is an incremental and iterative optimization algorithm to find values of parameters (weights) of a function that minimizes  cost function. \n",
    "- Least Squares method is a form of mathematical regression analysis that finds the line of best fit for a dataset, providing a - visual demonstration of the relationship between the data points.\n",
    "\n",
    "## Neural Network Approximating the Q Function\n",
    "\n",
    "![title](images/va_nonlinear.png)\n",
    "![title](images/va_nonlinear_z.png)\n",
    "\n",
    "\n",
    "## Q-Learning with Non-Linear Approximation\n",
    "\n",
    "- Step 1: Start with initial parameter values\n",
    "- Step 2: Take action a according to an explore or exploit policy, transitioning from s to s’\n",
    "- Step 3: Perform TD update for each parameter\n",
    "     \\begin{equation}\n",
    "\\large\n",
    "\\theta_i \\leftarrow \\theta_i + \\alpha [R(s) + \\beta * max_{a'}\\hat{Q_\\theta}(s', a') - \\hat{Q_\\theta}(s, a)]* \\frac{\\partial\\hat{Q_\\theta}(s,a)}{\\partial\\theta_i}\n",
    "\\end{equation}\n",
    "- Step 4: Go to Step 2\n",
    "\n",
    "Typically the space has many local minima and we no longer guarantee convergence, often works well in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Concepts\n",
    "\n",
    "- Perceptron, the first generation neural network, created a simple mathematical model or a function, mimicking neuron – the basic unit of brain\n",
    "- Sigmoid Neuron improved learning by giving some weightage to the input\n",
    "- Neural Network is a directed graph, organized by layers and layers are created by number of interconnected neurons (nodes)\n",
    "- Typical neural network contains three layers: input, hidden and output. If the hidden layers are more than one, then it is called deep neural network\n",
    "- Actual processing happens in hidden layers where each neuron acts as an activation function to process the input (from previous layers)\n",
    "- The performance of neural network is measured using cost or error function and the dependent weight functions\n",
    "- Forward and backward-propagation are two techniques, neural network users repeatedly until all the input variables are adjusted or calibrated to predict accurate output.\n",
    "- During, forward-propagation, information moves in forward direction and passes through all the layers by applying certain weights to the input parameters. Back-propagation method minimizes the error in the weights by applying an algorithm called gradient descent at each iteration step.\n",
    "\n",
    "![title](images/nn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook function to disable cell-level scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import gym\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from collections      import deque\n",
    "from keras.models     import Sequential\n",
    "from keras.layers     import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run util/flappy_bird_env_open_ai_gym.py #open AI gym clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueApproxSimpleNNModel(object):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, algorithm):\n",
    "        self.algorithm          = algorithm\n",
    "        self.learning_rate      = 0.001        \n",
    "        self.weight_backup      = \"flappy_va_{}.h5\".format(algorithm) \n",
    "        \n",
    "        self.state_size         = state_size        \n",
    "        self.action_size        = action_size    \n",
    "        \n",
    "        self.exploration_rate   = 1.0\n",
    "        self.exploration_min    = 0.01        \n",
    "        \n",
    "        self.brain              = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        #input layer\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        \n",
    "        #hidden layer\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        \n",
    "        #output layer with two outputs - up or down\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        #set the loss function\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        #check file exists to load the weights from\n",
    "        if os.path.isfile(self.weight_backup):\n",
    "            model.load_weights(self.weight_backup)\n",
    "            self.exploration_rate = self.exploration_min\n",
    "        return model\n",
    "\n",
    "    def save_model(self):\n",
    "            self.brain.save(self.weight_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueApproxAgent(object):\n",
    "    def __init__(self, state_size, action_size, model):        \n",
    "        self.state_size         = state_size\n",
    "        self.action_size        = action_size\n",
    "        self.memory             = deque(maxlen=2000)\n",
    "        self.learning_rate      = 0.001\n",
    "        self.gamma              = 0.95\n",
    "        self.exploration_rate   = 1.0\n",
    "        self.exploration_min    = 0.01\n",
    "        self.exploration_decay  = 0.995\n",
    "        self.model              = model        \n",
    "    \n",
    "    def act(self, state):\n",
    "        #*****************************************\n",
    "        #ACT: agent will randomly select its action at first by a certain percentage, \n",
    "        #called ‘exploration rate’ (or ‘epsilon’). \n",
    "        #At the beginning, it is better for the DQN agent to try \n",
    "        #different things before it starts to search for a pattern\n",
    "        #*****************************************\n",
    "        \n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.brain.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "   \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        #****************************************************\n",
    "         #One of the most important steps in the learning process is to remember \n",
    "         #what we did in the past and how the reward was bound to that action\n",
    "        #****************************************************\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    \n",
    "    def replay(self, sample_batch_size):\n",
    "        #*****************************************\n",
    "        #ONLINE learning from the samples of the execution trace\n",
    "        #REPLAY: Now that we have our past experiences in an array, \n",
    "        #we can train our neural network. \n",
    "        #We cannot afford to go through all our memory, it will take too many ressources. \n",
    "        #Therefore, we will only take a few samples (sample_batch_size and here set as 32) \n",
    "        #and we will just pick them randomly.\n",
    "        #*************************************************************\n",
    "        #not enough data to train; play another episode\n",
    "        if len(self.memory) < sample_batch_size:\n",
    "            return\n",
    "        \n",
    "        sample_batch = random.sample(self.memory, sample_batch_size)\n",
    "        for state, action, reward, next_state, done in sample_batch:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                #q-learning\n",
    "                target = reward + self.gamma * np.amax(self.model.brain.predict(next_state)[0])\n",
    "                \n",
    "            target_f = self.model.brain.predict(state)\n",
    "            target_f[0][action] = target\n",
    "                                    \n",
    "            #online learning with One Sample and discard this after fitting it to the model\n",
    "            self.model.brain.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        #adjust the exploration based on decay\n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate *= self.exploration_decay   \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueApproxGame:\n",
    "    \n",
    "    def __init__(self, env, agent, max_iterations= 10000):\n",
    "        \n",
    "        self.sample_batch_size = 32 #only few samples\n",
    "        self.episodes          = max_iterations\n",
    "        self.agent             = agent\n",
    "        self.env               = env #Open AI gym    \n",
    "        self.state_size        = self.env.observation_space.shape[0]\n",
    "        self.action_size       = self.env.action_space.n\n",
    "        \n",
    "        self.start             = datetime.datetime.now()      \n",
    "        self.data              = []\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        try:\n",
    "            for index_episode in range(self.episodes):\n",
    "                state = self.env.reset(2)\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "                done = False\n",
    "                index = 0\n",
    "                episode_reward = 0\n",
    "                \n",
    "                while not done:\n",
    "                    self.env.render(close = True)\n",
    "\n",
    "                    #take action\n",
    "                    action = self.agent.act(state)                    \n",
    "                    \n",
    "                    next_state, reward, done, _ = self.env.step(action, 2)\n",
    "                    next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                    \n",
    "                    self.agent.remember(state, action, reward, next_state, done)\n",
    "                    \n",
    "                    state = next_state\n",
    "                    episode_reward += reward\n",
    "                    index += 1\n",
    "                                \n",
    "                self.save_stats(index_episode, episode_reward, self.env.score)\n",
    "                \n",
    "                self.agent.model.save_model()\n",
    "                    \n",
    "                self.agent.replay(self.sample_batch_size)\n",
    "        finally:\n",
    "            self.agent.model.save_model()\n",
    "\n",
    "    #save_stats method is used to capture the output of all the episodes with metrics: \n",
    "    #algorithm, duration, episode, reward and score.**\n",
    "        \n",
    "    #only for the reporting purpose\n",
    "    def save_stats(self, episode, reward, score):\n",
    "                \n",
    "        duration = datetime.datetime.now() - self.start \n",
    "        \n",
    "        if (score >= 50):\n",
    "            print(\"Duration: {} Episode {} Score: {}\".format(duration, \n",
    "                                                                episode, \n",
    "                                                                score))\n",
    "        \n",
    "        self.data.append(json.dumps({ \"algorithm\": self.agent.model.algorithm, \n",
    "                    \"duration\":  \"{}\".format(duration), \n",
    "                    \"episode\":   episode, \n",
    "                    \"reward\":    reward, \n",
    "                    \"score\":     score}))\n",
    "        \n",
    "        if (len(self.data) == 500):\n",
    "            file_name = 'data/stats_flappy_bird_{}.json'.format(self.agent.model.algorithm)\n",
    "            \n",
    "            # delete the old file before saving data for this session\n",
    "            if episode == 1 and os.path.exists(file_name): os.remove(file_name)\n",
    "                \n",
    "            # open the file in append mode to add more json data\n",
    "            file = open(file_name, 'a+')  \n",
    "            for item in self.data:\n",
    "                file.write(item)  \n",
    "                file.write(\",\")\n",
    "            #end for\n",
    "            file.close()\n",
    "            \n",
    "            self.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    algorithm = \"Simple_Neural_Network\"\n",
    "    max_episodes = 10000\n",
    "    env = FlappyBirdEnv()\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    model = ValueApproxSimpleNNModel(state_size, action_size, algorithm)\n",
    "    agent = ValueApproxAgent(state_size, action_size, model)\n",
    "    \n",
    "    flappy = ValueApproxGame(env, agent, max_episodes)\n",
    "    \n",
    "    flappy.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
